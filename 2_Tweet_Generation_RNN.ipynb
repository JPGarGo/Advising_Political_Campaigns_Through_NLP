{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling and Text Generation using LSTMs — Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the sentiment analysis (in the previous notebook: 1_Sentiment_Analysis_Final), we will predict the future Tweets regarding some topics through Text Generation. With both tools combined, we will be able to really understand the level of sentiment (output opinion) for a certain person or a topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to go about Text Generation for Tweets: either use the standard NLP approach or the Deep Learning method.\n",
    "\n",
    "With the latest developments and improvements in the field of deep learning and artificial intelligence, many exacting tasks of Natural Language Processing are becoming facile to implement and execute. Text Generation is one such task which can be be architectured using deep learning models, particularly Recurrent Neural Networks (RNN).\n",
    "\n",
    "RNNs are used to, not only make predictions, but to also generate new possible sequences from a certain problem domain and by learning from the prior sequences. From this, we can also learn more about the problem itself.\n",
    "\n",
    "In this notebook we will cover the Deep Learning method within RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation for Tweets using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in class, Text Generation is a type of Language Modelling problem. Some of the tasks that require Languare Modelling are: speech to text, conversational systems and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. \n",
    "\n",
    "In this notebook, we will explain how to create a language model for generating natural language text by implementing and training Recurrent Neural Network. The goal is to ultimately compare the results of generating future Tweets with a RNN versus the simpler NLP (nltk) approach. We want to generate the new possible sequences of Tweets, based on prior Tweets about the 2016 GOP Debate. By predicting these Tweets, we will be able to understand some of the key opinions following this debate. \n",
    "\n",
    "We proceeded with this analysis by doing data cleaning and data preparation steps to really understand the power of the RNN in this context. However, we soon realized that, even with RNNs, the data needs to be well cleaned to generate good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview - Completed Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have divided our analysis in two part: we approached the problem in two ways. The first attempt describes the steps taken and the understanding to why some of the steps weren't helpful or needed extra work. The first step failed at generating text but gave us insight into what needed to be done. In the second attempt, we were capable of working in a systematic way and applying the required cleaning steps to successfully generate Tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Attempt at Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, we need to import the required libraries for the complete analysis. Keras, in language modeling, is an important library to import because it is a deep learning framework that contains other deep learning frameworks. It will be useful when creating our recurrent neural network. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "\n",
    "# set seeds for reproducability\n",
    "from tensorflow import set_random_seed\n",
    "from numpy.random import seed\n",
    "set_random_seed(2)\n",
    "seed(1)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "import os.path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, like in the others, we will be using a dataset that comprises of opinionated tweets related to presidential candidate speeches, during the 2016 GOP Debate, to train a text generation language model which can be used to generate further opiniated tweets on certain topics. \n",
    "\n",
    "We have saved the dataset into a csv file name Sentiment 4, from which we will input the column names mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(in_file):\n",
    "    my_path = os.getcwd()\n",
    "    path = os.path.join(my_path, in_file)\n",
    "    column_names = ['candidate','sentiment', 'subject', 'retweets', 'text', 'location', 'timezone']\n",
    "    tweets = pd.read_csv(path, delimiter=',', quotechar='\"', header= None, names= column_names, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    print('Readed ', len(tweets), \"tweets\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed  13871 tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1  RT @NancyLeeGrahn: How did everyone feel about...  Unknown   \n",
       "2  RT @ScottWalker: Didn't catch the full #GOPdeb...  Unknown   \n",
       "3  RT @TJMShow: No mention of Tamir Rice and the ...  Unknown   \n",
       "4  RT @RobGeorge: That Carly Fiorina is trending ...    Texas   \n",
       "5  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data = loadDataset(\"Sentiment_4.csv\")\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is comprised of 13871 Tweets. However, for our Text Generation, the only column we are interested in is the 'text' column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Preprocessing of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dataset cleaning step, we will first perform the text cleaning of the data which includes:\n",
    "- Removal of hashtags\n",
    "- Removal of usernames\n",
    "- Removal of URLs\n",
    "- Removal of the word gopdeb (word too common)\n",
    "- Removal of emoticons\n",
    "- Spliting of words by boundaries\n",
    "- Removal of punctuation\n",
    "- Removal of repetitive words\n",
    "- Stemming of words\n",
    "\n",
    "In this step, we will start by creating all the functions to cleaning the text. We will then apply all the functions in one wrapper (at the same time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to normal texts, tweets have more dirty data. Therefore, we need to make sure to take out additional noise (hashtags, usernames, URLs) before applying traditional NLP cleaning (like taught in class). To do so, we used regular expressions (regex)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hashtags\n",
    "import re\n",
    "hash_regex = re.compile(r\"#(\\w+)\")\n",
    "def hash_repl(match):\n",
    "\treturn ''+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usernames\n",
    "user_regex = re.compile(r\"@(\\w+)\")\n",
    "def user_repl(match):\n",
    "\treturn ''+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL\n",
    "url_regex = re.compile(r\"(http|https|ftp)://[a-zA-Z0-9\\./]+\")\n",
    "def url_repl(match):\n",
    "\treturn ''+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoticons\n",
    "emoticons = \\\n",
    "\t[\t('__EMOT_SMILEY',\t[':-)', ':)', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_LAUGH',\t\t[':-D', ':D', 'X-D', 'XD', 'xD', ] )\t,\\\n",
    "\t\t('__EMOT_LOVE',\t\t['<3', ':\\*', ] )\t,\\\n",
    "\t\t('__EMOT_WINK',\t\t[';-)', ';)', ';-D', ';D', '(;', '(-;', ] )\t,\\\n",
    "\t\t('__EMOT_FROWN',\t\t[':-(', ':(', '(:', '(-:', ] )\t,\\\n",
    "\t\t('__EMOT_CRY',\t\t[':,(', ':\\'(', ':\"(', ':(('] )\t,\\\n",
    "\t]\n",
    "    \n",
    "def escape_paren(arr):\n",
    "\treturn [text.replace(')', '[)}\\]]').replace('(', '[({\\[]') for text in arr]\n",
    "\n",
    "def regex_union(arr):\n",
    "\treturn '(' + '|'.join( arr ) + ')'\n",
    "\n",
    "emoticons_regex = [ (repl, re.compile(regex_union(escape_paren(regx))) ) for (repl, regx) in emoticons ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the functions for removing dirty data are completed, we can apply the traditional NLP approach for cleaning the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of the Word Gopdeb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the word geopdeb because it was the most frequent string value used to query the data and was often used following a hashtag. By taking out this word, we would \"standardize\" the diversity and proportion of words in the tweets. \n",
    "\n",
    "We did the same for other repetitive words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common word\n",
    "word_regex = re.compile(r\"\\bgopdeb\\b\\s+\")\n",
    "def word_repl(match):\n",
    "\treturn ''+match.group(1).upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removal of repetitive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating words (like hurrrryyyyyy)\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE);\n",
    "def rpt_repl(match):\n",
    "\treturn match.group(1)+match.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting of words by boundaries and Removal of Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also removed punctuation, which are often not as present in tweets. Still, we wanted to eliminate the noise, as the tweets generated will not require punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting by word boundaries\n",
    "word_bound_regex = re.compile(r\"\\W+\")\n",
    "\n",
    "# Punctuations\n",
    "punctuations = \\\n",
    "\t[\t#('',\t\t['.', ] )\t,\\\n",
    "\t\t#('',\t\t[',', ] )\t,\\\n",
    "\t\t#('',\t\t['\\'', '\\\"', ] )\t,\\\n",
    "\t\t('',\t\t['!', '¡', ] )\t,\\\n",
    "\t\t('__PUNC_QUES',\t\t['?', '¿', ] )\t,\\\n",
    "\t\t('__PUNC_ELLP',\t\t['...', '…', ] )\t,\\\n",
    "\t]\n",
    "\n",
    "#For punctuation replacement\n",
    "def punctuations_repl(match):\n",
    "\ttext = match.group(0)\n",
    "\trepl = []\n",
    "\tfor (key, parr) in punctuations :\n",
    "\t\tfor punc in parr :\n",
    "\t\t\tif punc in text:\n",
    "\t\t\t\trepl.append(key)\n",
    "\tif( len(repl)>0 ) :\n",
    "\t\treturn ' '+' '.join(repl)+' '\n",
    "\telse :\n",
    "\t\treturn ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we apply stemming in order to standardize the text and consider the different word variants as one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "#Porter Stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining functions and applying them to the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we combine all the previous functions in one wrapper, to which we will apply to the text column and create a new column, named text_processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function that encloses all the processing procedures\n",
    "def processAll(text):\n",
    "    \n",
    "    text = re.sub( hash_regex, hash_repl, text )\n",
    "    text = re.sub( user_regex, user_repl, text)\n",
    "    text = re.sub( url_regex, url_repl, text )\n",
    "    text = re.sub( word_regex, word_repl, text)\n",
    "    \n",
    "    for (repl, regx) in emoticons_regex :\n",
    "        text = re.sub(regx, ' '+repl+' ', text)\n",
    "    \n",
    "    text = text.replace('\\'','')\n",
    "    \n",
    "    text = re.sub( word_bound_regex , punctuations_repl, text )\n",
    "    text = re.sub( rpt_regex, rpt_repl, text )\n",
    "    \n",
    "        \n",
    "    text = [word if(word[0:2]=='__') else word.lower() for word in text.split() if len(word) >= 3]\n",
    "    text = [stemmer.stem(w) for w in text]                \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "      <td>[nancyleegrahn, how, did, everyon, feel, about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[scottwalk, didnt, catch, the, full, gopdeb, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[tjmshow, mention, tamir, rice, and, the, gopd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>[robgeorg, that, carli, fiorina, trend, hour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>[danscavino, gopdeb, realdonaldtrump, deliv, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1  RT @NancyLeeGrahn: How did everyone feel about...  Unknown   \n",
       "2  RT @ScottWalker: Didn't catch the full #GOPdeb...  Unknown   \n",
       "3  RT @TJMShow: No mention of Tamir Rice and the ...  Unknown   \n",
       "4  RT @RobGeorge: That Carly Fiorina is trending ...    Texas   \n",
       "5  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Unknown   \n",
       "\n",
       "                     timezone  \\\n",
       "1                       Quito   \n",
       "2                     Unknown   \n",
       "3                     Unknown   \n",
       "4  Central Time (US & Canada)   \n",
       "5                     Arizona   \n",
       "\n",
       "                                      text_processed  \n",
       "1  [nancyleegrahn, how, did, everyon, feel, about...  \n",
       "2  [scottwalk, didnt, catch, the, full, gopdeb, l...  \n",
       "3  [tjmshow, mention, tamir, rice, and, the, gopd...  \n",
       "4  [robgeorg, that, carli, fiorina, trend, hour, ...  \n",
       "5  [danscavino, gopdeb, realdonaldtrump, deliv, t...  "
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data['text_processed'] = raw_training_data.text.apply(processAll)\n",
    "raw_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the amount of noise, we decided to keep only the column text_processed, which is the new text column with all the data cleaning applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [nancyleegrahn, how, did, everyon, feel, about...\n",
       "2    [scottwalk, didnt, catch, the, full, gopdeb, l...\n",
       "3    [tjmshow, mention, tamir, rice, and, the, gopd...\n",
       "4    [robgeorg, that, carli, fiorina, trend, hour, ...\n",
       "5    [danscavino, gopdeb, realdonaldtrump, deliv, t...\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines = raw_training_data['text_processed']\n",
    "all_headlines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following all these steps, we can see that the data was cleaned. From there, we will start preparing our data to be ingested in the RNN, which we will also create. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will start by merging all the processed text (arrays) in one array (named corpus). Merging all this information into one corpus allows us to feed the corpus, later on, in the recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nancyleegrahn',\n",
       "  'how',\n",
       "  'did',\n",
       "  'everyon',\n",
       "  'feel',\n",
       "  'about',\n",
       "  'the',\n",
       "  'climat',\n",
       "  'chang',\n",
       "  'question',\n",
       "  'last',\n",
       "  'night',\n",
       "  '__punc_qu',\n",
       "  'exactli',\n",
       "  'gopdeb'],\n",
       " ['scottwalk',\n",
       "  'didnt',\n",
       "  'catch',\n",
       "  'the',\n",
       "  'full',\n",
       "  'gopdeb',\n",
       "  'last',\n",
       "  'night',\n",
       "  'here',\n",
       "  'are',\n",
       "  'some',\n",
       "  'scott',\n",
       "  'best',\n",
       "  'line',\n",
       "  'second',\n",
       "  'walker16',\n",
       "  'httpâ'],\n",
       " ['tjmshow',\n",
       "  'mention',\n",
       "  'tamir',\n",
       "  'rice',\n",
       "  'and',\n",
       "  'the',\n",
       "  'gopdeb',\n",
       "  'wa',\n",
       "  'held',\n",
       "  'cleveland',\n",
       "  '__punc_qu',\n",
       "  'wow'],\n",
       " ['robgeorg',\n",
       "  'that',\n",
       "  'carli',\n",
       "  'fiorina',\n",
       "  'trend',\n",
       "  'hour',\n",
       "  'after',\n",
       "  'her',\n",
       "  'debat',\n",
       "  'abov',\n",
       "  'ani',\n",
       "  'the',\n",
       "  'men',\n",
       "  'just',\n",
       "  'complet',\n",
       "  'gopdeb',\n",
       "  'say',\n",
       "  'she'],\n",
       " ['danscavino',\n",
       "  'gopdeb',\n",
       "  'realdonaldtrump',\n",
       "  'deliv',\n",
       "  'the',\n",
       "  'highest',\n",
       "  'rate',\n",
       "  'the',\n",
       "  'histori',\n",
       "  'presidenti',\n",
       "  'debat',\n",
       "  'trump2016',\n",
       "  'httpâ'],\n",
       " ['gregabbott_tx',\n",
       "  'tedcruz',\n",
       "  'first',\n",
       "  'day',\n",
       "  'will',\n",
       "  'rescind',\n",
       "  'everi',\n",
       "  'illeg',\n",
       "  'execut',\n",
       "  'action',\n",
       "  'taken',\n",
       "  'barack',\n",
       "  'obama',\n",
       "  'gopdeb',\n",
       "  'foxnew'],\n",
       " ['warriorwoman91',\n",
       "  'like',\n",
       "  'her',\n",
       "  'and',\n",
       "  'wa',\n",
       "  'happi',\n",
       "  'when',\n",
       "  'heard',\n",
       "  'she',\n",
       "  'wa',\n",
       "  'go',\n",
       "  'the',\n",
       "  'moder',\n",
       "  'not',\n",
       "  'anymor',\n",
       "  'gopdeb',\n",
       "  'megynkelli',\n",
       "  'http'],\n",
       " ['go', 'msnbc', 'live', 'with', 'thomasarobert', 'around', 'gopdeb'],\n",
       " ['deer',\n",
       "  'the',\n",
       "  'headlight',\n",
       "  'lizzwinstead',\n",
       "  'ben',\n",
       "  'carson',\n",
       "  'may',\n",
       "  'the',\n",
       "  'onli',\n",
       "  'brain',\n",
       "  'surgeon',\n",
       "  'who',\n",
       "  'ha',\n",
       "  'perform',\n",
       "  'lobotomi',\n",
       "  'himself',\n",
       "  'gopdeb'],\n",
       " ['nancyosborne180',\n",
       "  'last',\n",
       "  'night',\n",
       "  'debat',\n",
       "  'prove',\n",
       "  'gopdeb',\n",
       "  'batsask',\n",
       "  'badassteachersa',\n",
       "  'tbat',\n",
       "  'http']]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sequence of N-Grams Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language modelling requires a sequence input data, as given a sequence (of words/tokens) where the aim is to predict the next word/token.\n",
    "\n",
    "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Python’s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens.\n",
    "\n",
    "Next, we need to convert the corpus into a flat dataset of sentence sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2192, 42],\n",
       " [2192, 42, 67],\n",
       " [2192, 42, 67, 283],\n",
       " [2192, 42, 67, 283, 346],\n",
       " [2192, 42, 67, 283, 346, 17],\n",
       " [2192, 42, 67, 283, 346, 17, 2],\n",
       " [2192, 42, 67, 283, 346, 17, 2, 399],\n",
       " [2192, 42, 67, 283, 346, 17, 2, 399, 252],\n",
       " [2192, 42, 67, 283, 346, 17, 2, 399, 252, 33],\n",
       " [2192, 42, 67, 283, 346, 17, 2, 399, 252, 33, 25]]"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the Sequences and obtain Variables : Predictors and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated a data-set which contains sequence of tokens, it is possible that different sequences have different lengths. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Keras for this purpose. To input this data into a learning model, we need to create predictors and label. We created N-grams sequences as predictors and the next word of the N-gram as label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, we can obtain the input vector X and the label vector Y which can be used for the training purposes. Recent experiments have shown that recurrent neural networks have shown a good performance in sequence to sequence learning and text data applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTMs for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
    "\n",
    "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
    "\n",
    "LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. Here are the layers in our model:\n",
    "\n",
    "- Input Layer : Takes the sequence of words as input\n",
    "- LSTM Layer : Computes the output using LSTM units (number can be tuned)\n",
    "- Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting\n",
    "- Output Layer : Computes the probability of the best possible next word as output\n",
    "\n",
    "We will also run this model for total 1 and 100 epochs to analyze the difference in results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 25, 10)            127080    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 12708)             1283508   \n",
      "=================================================================\n",
      "Total params: 1,454,988\n",
      "Trainable params: 1,454,988\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model with 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "177564/177564 [==============================] - 474s 3ms/step - loss: 6.9558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a7332ec18>"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=1, verbose=1)\n",
    "#don't want to run on 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the model with 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Camille Blain-Coalli\\Downloads\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "217521/217521 [==============================] - 750s 3ms/step - loss: 7.5265\n",
      "Epoch 2/100\n",
      "217521/217521 [==============================] - 748s 3ms/step - loss: 7.4725\n",
      "Epoch 3/100\n",
      "217521/217521 [==============================] - 757s 3ms/step - loss: 7.4743\n",
      "Epoch 4/100\n",
      "217521/217521 [==============================] - 764s 4ms/step - loss: 7.4736\n",
      "Epoch 5/100\n",
      "217521/217521 [==============================] - 762s 4ms/step - loss: 7.4758\n",
      "Epoch 6/100\n",
      "217521/217521 [==============================] - 751s 3ms/step - loss: 7.4768\n",
      "Epoch 7/100\n",
      "217521/217521 [==============================] - 748s 3ms/step - loss: 7.4778\n",
      "Epoch 8/100\n",
      "217521/217521 [==============================] - 715s 3ms/step - loss: 7.4777\n",
      "Epoch 9/100\n",
      "217521/217521 [==============================] - 758s 3ms/step - loss: 7.4772\n",
      "Epoch 10/100\n",
      "217521/217521 [==============================] - 742s 3ms/step - loss: 7.4792\n",
      "Epoch 11/100\n",
      "217521/217521 [==============================] - 720s 3ms/step - loss: 7.4803\n",
      "Epoch 12/100\n",
      "217521/217521 [==============================] - 519s 2ms/step - loss: 7.4800\n",
      "Epoch 13/100\n",
      "217521/217521 [==============================] - 513s 2ms/step - loss: 7.4818\n",
      "Epoch 14/100\n",
      "217521/217521 [==============================] - 569s 3ms/step - loss: 7.4810\n",
      "Epoch 15/100\n",
      "217521/217521 [==============================] - 555s 3ms/step - loss: 7.4808\n",
      "Epoch 16/100\n",
      "217521/217521 [==============================] - 522s 2ms/step - loss: 7.4825\n",
      "Epoch 17/100\n",
      "217521/217521 [==============================] - 510s 2ms/step - loss: 7.4822\n",
      "Epoch 18/100\n",
      "217521/217521 [==============================] - 540s 2ms/step - loss: 7.4827\n",
      "Epoch 19/100\n",
      "217521/217521 [==============================] - 571s 3ms/step - loss: 7.4829\n",
      "Epoch 20/100\n",
      "217521/217521 [==============================] - 669s 3ms/step - loss: 7.4843\n",
      "Epoch 21/100\n",
      "217521/217521 [==============================] - 689s 3ms/step - loss: 7.4839\n",
      "Epoch 22/100\n",
      "217521/217521 [==============================] - 737s 3ms/step - loss: 7.4832\n",
      "Epoch 23/100\n",
      "217521/217521 [==============================] - 685s 3ms/step - loss: 7.4858\n",
      "Epoch 24/100\n",
      "217521/217521 [==============================] - 718s 3ms/step - loss: 7.4849\n",
      "Epoch 25/100\n",
      "217521/217521 [==============================] - 712s 3ms/step - loss: 7.4858\n",
      "Epoch 26/100\n",
      "217521/217521 [==============================] - 698s 3ms/step - loss: 7.4860\n",
      "Epoch 27/100\n",
      "217521/217521 [==============================] - 748s 3ms/step - loss: 7.4870\n",
      "Epoch 28/100\n",
      "217521/217521 [==============================] - 717s 3ms/step - loss: 7.4872\n",
      "Epoch 29/100\n",
      "217521/217521 [==============================] - 696s 3ms/step - loss: 7.4874\n",
      "Epoch 30/100\n",
      "217521/217521 [==============================] - 918s 4ms/step - loss: 7.4862\n",
      "Epoch 32/100\n",
      "217521/217521 [==============================] - 597s 3ms/step - loss: 7.4893\n",
      "Epoch 33/100\n",
      "217521/217521 [==============================] - 561s 3ms/step - loss: 7.4895\n",
      "Epoch 34/100\n",
      "217521/217521 [==============================] - 553s 3ms/step - loss: 7.4902\n",
      "Epoch 35/100\n",
      "217521/217521 [==============================] - 547s 3ms/step - loss: 7.4901\n",
      "Epoch 36/100\n",
      "217521/217521 [==============================] - 524s 2ms/step - loss: 7.4894\n",
      "Epoch 37/100\n",
      "217521/217521 [==============================] - 537s 2ms/step - loss: 7.4896\n",
      "Epoch 38/100\n",
      "217521/217521 [==============================] - 552s 3ms/step - loss: 7.4907\n",
      "Epoch 39/100\n",
      "217521/217521 [==============================] - 517s 2ms/step - loss: 7.4892\n",
      "Epoch 40/100\n",
      "217521/217521 [==============================] - 535s 2ms/step - loss: 7.4899\n",
      "Epoch 41/100\n",
      "217521/217521 [==============================] - 512s 2ms/step - loss: 7.4902\n",
      "Epoch 42/100\n",
      "217521/217521 [==============================] - 538s 2ms/step - loss: 7.4903\n",
      "Epoch 43/100\n",
      "217521/217521 [==============================] - 494s 2ms/step - loss: 7.4901\n",
      "Epoch 44/100\n",
      "217521/217521 [==============================] - 753s 3ms/step - loss: 7.4916\n",
      "Epoch 46/100\n",
      "217521/217521 [==============================] - 752s 3ms/step - loss: 7.4905\n",
      "Epoch 47/100\n",
      "217521/217521 [==============================] - 709s 3ms/step - loss: 7.4914\n",
      "Epoch 48/100\n",
      "217521/217521 [==============================] - 739s 3ms/step - loss: 7.4920\n",
      "Epoch 49/100\n",
      "217521/217521 [==============================] - 738s 3ms/step - loss: 7.4929\n",
      "Epoch 50/100\n",
      "217521/217521 [==============================] - 818s 4ms/step - loss: 7.4909\n",
      "Epoch 51/100\n",
      "217521/217521 [==============================] - 764s 4ms/step - loss: 7.4919\n",
      "Epoch 52/100\n",
      "217521/217521 [==============================] - 692s 3ms/step - loss: 7.4907\n",
      "Epoch 53/100\n",
      "217521/217521 [==============================] - 651s 3ms/step - loss: 7.4923\n",
      "Epoch 54/100\n",
      "217521/217521 [==============================] - 603s 3ms/step - loss: 7.4933\n",
      "Epoch 55/100\n",
      "217521/217521 [==============================] - 570s 3ms/step - loss: 7.4914\n",
      "Epoch 56/100\n",
      "217521/217521 [==============================] - 577s 3ms/step - loss: 7.4931\n",
      "Epoch 57/100\n",
      "217521/217521 [==============================] - 579s 3ms/step - loss: 7.4929\n",
      "Epoch 58/100\n",
      "217521/217521 [==============================] - 538s 2ms/step - loss: 7.4929 1\n",
      "Epoch 59/100\n",
      "217521/217521 [==============================] - 510s 2ms/step - loss: 7.4916\n",
      "Epoch 60/100\n",
      "217521/217521 [==============================] - 519s 2ms/step - loss: 7.4933\n",
      "Epoch 61/100\n",
      "217521/217521 [==============================] - 521s 2ms/step - loss: 7.4924\n",
      "Epoch 62/100\n",
      "217521/217521 [==============================] - 534s 2ms/step - loss: 7.4940\n",
      "Epoch 63/100\n",
      "217521/217521 [==============================] - 534s 2ms/step - loss: 7.4936\n",
      "Epoch 64/100\n",
      "217521/217521 [==============================] - 535s 2ms/step - loss: 7.4935\n",
      "Epoch 65/100\n",
      "217521/217521 [==============================] - 507s 2ms/step - loss: 7.4924\n",
      "Epoch 66/100\n",
      "217521/217521 [==============================] - 512s 2ms/step - loss: 7.4937\n",
      "Epoch 67/100\n",
      "217521/217521 [==============================] - 536s 2ms/step - loss: 7.4942\n",
      "Epoch 68/100\n",
      "217521/217521 [==============================] - 533s 2ms/step - loss: 7.4928\n",
      "Epoch 69/100\n",
      "217521/217521 [==============================] - 526s 2ms/step - loss: 7.4930\n",
      "Epoch 70/100\n",
      "217521/217521 [==============================] - 522s 2ms/step - loss: 7.4949\n",
      "Epoch 71/100\n",
      "217521/217521 [==============================] - 522s 2ms/step - loss: 7.4930\n",
      "Epoch 72/100\n",
      "217521/217521 [==============================] - 523s 2ms/step - loss: 7.4952\n",
      "Epoch 73/100\n",
      "217521/217521 [==============================] - 541s 2ms/step - loss: 7.4939\n",
      "Epoch 74/100\n",
      "217521/217521 [==============================] - 534s 2ms/step - loss: 7.4949\n",
      "Epoch 75/100\n",
      "217521/217521 [==============================] - 536s 2ms/step - loss: 7.4944\n",
      "Epoch 76/100\n",
      "217521/217521 [==============================] - 550s 3ms/step - loss: 7.4939\n",
      "Epoch 77/100\n",
      "217521/217521 [==============================] - 519s 2ms/step - loss: 7.4957\n",
      "Epoch 78/100\n",
      "217521/217521 [==============================] - 527s 2ms/step - loss: 7.4936\n",
      "Epoch 79/100\n",
      "217521/217521 [==============================] - 542s 2ms/step - loss: 7.4943\n",
      "Epoch 80/100\n",
      "217521/217521 [==============================] - 522s 2ms/step - loss: 7.4951\n",
      "Epoch 81/100\n",
      "217521/217521 [==============================] - 549s 3ms/step - loss: 7.4951\n",
      "Epoch 82/100\n",
      "217521/217521 [==============================] - 523s 2ms/step - loss: 7.4958\n",
      "Epoch 83/100\n",
      "217521/217521 [==============================] - 506s 2ms/step - loss: 7.4948\n",
      "Epoch 84/100\n",
      "217521/217521 [==============================] - 515s 2ms/step - loss: 7.4953\n",
      "Epoch 85/100\n",
      "217521/217521 [==============================] - 526s 2ms/step - loss: 7.4943\n",
      "Epoch 86/100\n",
      "217521/217521 [==============================] - 511s 2ms/step - loss: 7.4960\n",
      "Epoch 87/100\n",
      "217521/217521 [==============================] - 510s 2ms/step - loss: 7.4941\n",
      "Epoch 88/100\n",
      "217521/217521 [==============================] - 532s 2ms/step - loss: 7.4961\n",
      "Epoch 89/100\n",
      "217521/217521 [==============================] - 487s 2ms/step - loss: 7.4940\n",
      "Epoch 90/100\n",
      "217521/217521 [==============================] - 507s 2ms/step - loss: 7.4956\n",
      "Epoch 91/100\n",
      "217521/217521 [==============================] - 495s 2ms/step - loss: 7.4953 0s - loss: 7.49\n",
      "Epoch 92/100\n",
      "217521/217521 [==============================] - 486s 2ms/step - loss: 7.4959\n",
      "Epoch 93/100\n",
      "217521/217521 [==============================] - 487s 2ms/step - loss: 7.4963\n",
      "Epoch 94/100\n",
      "217521/217521 [==============================] - 490s 2ms/step - loss: 7.4964\n",
      "Epoch 95/100\n",
      "217521/217521 [==============================] - 495s 2ms/step - loss: 7.4962\n",
      "Epoch 96/100\n",
      "217521/217521 [==============================] - 511s 2ms/step - loss: 7.4956\n",
      "Epoch 97/100\n",
      "217521/217521 [==============================] - 518s 2ms/step - loss: 7.4956\n",
      "Epoch 98/100\n",
      "217521/217521 [==============================] - 489s 2ms/step - loss: 7.4956\n",
      "Epoch 99/100\n",
      "217521/217521 [==============================] - 497s 2ms/step - loss: 7.4959\n",
      "Epoch 100/100\n",
      "217521/217521 [==============================] - 500s 2ms/step - loss: 7.4975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a4532feb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=1)\n",
    "#don't want to run on 100 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are creating the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our results, we are comparing running this model through 1 epoch and 100 epoch. we have asked the model to predict the 5 words following a specific word we have identified (in this case: Trump). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Gopdeb Gopdeb Gopdeb Gopdeb Gopdeb\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Trump\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results for 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Gopdebate Gopdebate Gopdebate Gopdebate Gopdebate\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Trump\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we generate results that are all the same (5 predicted words). In addition, increasing the number of training epochs doesn't help our model. The model is not capable of generating a languague model, even with 100 epochs (that ran for over 12 hours). In addition, the resulting word is Gopdebate or Gopdeb, thus, we can infer that the words weren't filtered out completely and that having them present in the dataset, skews the results. \n",
    "\n",
    "In our second part of this notebook, we will rework the steps to have an effective and useful cleaning process to the data, hoping this will correct the problem above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Attempt at Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid overwriting the above results, we will load the same dataset once again, and create a new dataframe to work from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(in_file):\n",
    "    my_path = os.getcwd()\n",
    "    path = os.path.join(my_path, in_file)\n",
    "    column_names = ['candidate','sentiment', 'subject', 'retweets', 'text', 'location', 'timezone']\n",
    "    tweets = pd.read_csv(path, delimiter=',', quotechar='\"', header= None, names= column_names, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    print('Readed ', len(tweets), \"tweets\")\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readed  13871 tweets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1  RT @NancyLeeGrahn: How did everyone feel about...  Unknown   \n",
       "2  RT @ScottWalker: Didn't catch the full #GOPdeb...  Unknown   \n",
       "3  RT @TJMShow: No mention of Tamir Rice and the ...  Unknown   \n",
       "4  RT @RobGeorge: That Carly Fiorina is trending ...    Texas   \n",
       "5  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data2 = loadDataset(\"Sentiment_4.csv\")\n",
    "raw_training_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we recall from the first part, we decided to apply all the cleaning functions in one wrapping (all at once) data cleaning step. \n",
    "\n",
    "In this second try at the data, we understand the importance of correctly cleaning the dataset. Thus, we will systematically clean the dataset, one step at a time, analyzing the impact of each function. This will allow us to have a better overview at the changes made and if they were effective in the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords and Apply Lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, even when taking out gopdebate, we cannot eliminate all mentions of this word because the text is case sentivitive. Therefore, we convert all string values (in the text column) to lowercase to avoid this problem. \n",
    "\n",
    "We also decide to eliminate stopwords because Tweets normally contain less words. Because of that, some tweets may be composed of mostly stopwords, which may not give a good output value to our Language Modelling. We decide to remove them so they don't impact the final result. Plus, the idea in the tweet generation is to get an idea of the opinion on certain topics, not necessarily a correct grammar case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>rt @nancyleegrahn: everyone feel climate chang...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>rt @scottwalker: catch full #gopdebate last ni...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>rt @tjmshow: mention tamir rice #gopdebate hel...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>rt @robgeorge: carly fiorina trending -- hours...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>rt @danscavino: #gopdebate w/ @realdonaldtrump...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1  rt @nancyleegrahn: everyone feel climate chang...  Unknown   \n",
       "2  rt @scottwalker: catch full #gopdebate last ni...  Unknown   \n",
       "3  rt @tjmshow: mention tamir rice #gopdebate hel...  Unknown   \n",
       "4  rt @robgeorge: carly fiorina trending -- hours...    Texas   \n",
       "5  rt @danscavino: #gopdebate w/ @realdonaldtrump...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    return \" \".join(text)\n",
    "\n",
    "raw_training_data2['text'] = raw_training_data2['text'].apply(stopwords)\n",
    "raw_training_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Common Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When further analyzing the dataset, we can see that all (or almost all) tweets in the text column start the with word 'rt'. This is an acronynm for retweet and is irrelevant in determing the future Tweets. We can also see, further in the string values, that the words gopdeb, gopdebate and gopdebat are used. So, we eliminate both words at this instance. By this, we hope to avoid skewing the results to more popular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>@nancyleegrahn: everyone feel climate change ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>@scottwalker: catch full #gopdebate last nigh...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>@tjmshow: mention tamir rice #gopdebate held ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>@robgeorge: carly fiorina trending -- hours d...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>@danscavino: #gopdebate w/ @realdonaldtrump d...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1   @nancyleegrahn: everyone feel climate change ...  Unknown   \n",
       "2   @scottwalker: catch full #gopdebate last nigh...  Unknown   \n",
       "3   @tjmshow: mention tamir rice #gopdebate held ...  Unknown   \n",
       "4   @robgeorge: carly fiorina trending -- hours d...    Texas   \n",
       "5   @danscavino: #gopdebate w/ @realdonaldtrump d...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data2['text'] = raw_training_data2['text'].map(lambda x: x.lstrip('rt').rstrip('gopdeb'))\n",
    "raw_training_data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>@nancyleegrahn: everyone feel climate change ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>@scottwalker: catch full # last night. scott'...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>@tjmshow: mention tamir rice # held cleveland...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>@robgeorge: carly fiorina trending -- hours d...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>@danscavino: # w/ @realdonaldtrump delivered ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1   @nancyleegrahn: everyone feel climate change ...  Unknown   \n",
       "2   @scottwalker: catch full # last night. scott'...  Unknown   \n",
       "3   @tjmshow: mention tamir rice # held cleveland...  Unknown   \n",
       "4   @robgeorge: carly fiorina trending -- hours d...    Texas   \n",
       "5   @danscavino: # w/ @realdonaldtrump delivered ...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data2['text'] = raw_training_data2.text.str.replace(\"gopdebate\", \"\")\n",
    "raw_training_data2['text'] = raw_training_data2.text.str.replace(\"gopdebat\", \"\")\n",
    "raw_training_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Words with Only 1 Letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering we have eliminated all stopwords, in this step, we decide to remove all words of one letter because those words represent words that (most likely) don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subject</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>5</td>\n",
       "      <td>@nancyleegrahn: everyone feel climate change ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>26</td>\n",
       "      <td>@scottwalker: catch full # last night. scott'...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>27</td>\n",
       "      <td>@tjmshow: mention tamir rice # held cleveland...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>138</td>\n",
       "      <td>@robgeorge: carly fiorina trending -- hours d...</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Positive</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>156</td>\n",
       "      <td>@danscavino: # / @realdonaldtrump delivered h...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                candidate sentiment            subject  retweets  \\\n",
       "1  No candidate mentioned   Neutral  None of the above         5   \n",
       "2            Scott Walker  Positive  None of the above        26   \n",
       "3  No candidate mentioned   Neutral  None of the above        27   \n",
       "4  No candidate mentioned  Positive  None of the above       138   \n",
       "5            Donald Trump  Positive  None of the above       156   \n",
       "\n",
       "                                                text location  \\\n",
       "1   @nancyleegrahn: everyone feel climate change ...  Unknown   \n",
       "2   @scottwalker: catch full # last night. scott'...  Unknown   \n",
       "3   @tjmshow: mention tamir rice # held cleveland...  Unknown   \n",
       "4   @robgeorge: carly fiorina trending -- hours d...    Texas   \n",
       "5   @danscavino: # / @realdonaldtrump delivered h...  Unknown   \n",
       "\n",
       "                     timezone  \n",
       "1                       Quito  \n",
       "2                     Unknown  \n",
       "3                     Unknown  \n",
       "4  Central Time (US & Canada)  \n",
       "5                     Arizona  "
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_training_data2['text'] = raw_training_data2.text.str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "raw_training_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to reduce the amount of noise, we decided to keep only the column text, which is the text column now modified with all the data cleaning applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     @nancyleegrahn: everyone feel climate change ...\n",
       "2     @scottwalker: catch full # last night. scott'...\n",
       "3     @tjmshow: mention tamir rice # held cleveland...\n",
       "4     @robgeorge: carly fiorina trending -- hours d...\n",
       "5     @danscavino: # / @realdonaldtrump delivered h...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_headlines2 = raw_training_data2['text']\n",
    "all_headlines2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we will combine, again, all the text in one array, we will apply further steps to the data cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' @nancyleegrahn: everyone feel climate change question last night? exactly. #',\n",
       " \" @scottwalker: catch full # last night. scott' best lines 90 seconds. #walker16 http://.co/zsff\",\n",
       " ' @tjmshow: mention tamir rice # held cleveland? wow.',\n",
       " ' @robgeorge: carly fiorina trending -- hours debate -- men just-completed # says ',\n",
       " ' @danscavino: # / @realdonaldtrump delivered highest ratings history presidential debates. #trump2016 http://.co',\n",
       " ' @gregabbott_tx: @tedcruz: \"on first day rescind every illegal executive action taken barack obama.\" # @foxnews',\n",
       " ' @warriorwoman91: liked happy heard going moderator. anymore. # @megynkelly https://',\n",
       " 'going #msnbc live @thomasaroberts around pm et. #',\n",
       " 'deer headlights rt @lizzwinstead: ben carson, may brain surgeon performed lobotomy himself. #',\n",
       " \" @nancyosborne180: last night' debate proved it! # #batsask @badassteachersa #tbats https://.co/g2ggjy1bj\"]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [clean_text(x) for x in all_headlines2]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' @nancyleegrahn: everyone feel climate change question last night? exactly. #',\n",
       " \" @scottwalker: catch full # last night. scott' best lines 90 seconds. #walker16 http://.co/zsff\",\n",
       " ' @tjmshow: mention tamir rice # held cleveland? wow.',\n",
       " ' @robgeorge: carly fiorina trending -- hours debate -- men just-completed # says ',\n",
       " ' @danscavino: # / @realdonaldtrump delivered highest ratings history presidential debates. #trump2016 http://.co',\n",
       " ' @gregabbott_tx: @tedcruz: \"on first day rescind every illegal executive action taken barack obama.\" # @foxnews',\n",
       " ' @warriorwoman91: liked happy heard going moderator. anymore. # @megynkelly https://',\n",
       " 'going #msnbc live @thomasaroberts around pm et. #',\n",
       " 'deer headlights rt @lizzwinstead: ben carson, may brain surgeon performed lobotomy himself. #',\n",
       " \" @nancyosborne180: last night' debate proved it! # #batsask @badassteachersa #tbats https://.co/g2ggjy1bj\"]"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines2]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Symbols, Words and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' @nancyleegrahn: everyone feel climate change question last night? exactly. #',\n",
       " \" @scottwalker: catch full # last night. scott' best lines 90 seconds. #walker16 , \",\n",
       " ' @tjmshow: mention tamir rice # held cleveland? wow.',\n",
       " ' @robgeorge: carly fiorina trending -- hours debate -- men just-completed # says ',\n",
       " ' @danscavino: # / @realdonaldtrump delivered highest ratings history presidential debates. #trump2016 , ',\n",
       " ' @gregabbott_tx: @tedcruz: \"on first day rescind every illegal executive action taken barack obama.\" # @foxnews',\n",
       " ' @warriorwoman91: liked happy heard going moderator. anymore. # @megynkelly , ',\n",
       " 'going #msnbc live @thomasaroberts around pm et. #',\n",
       " 'deer headlights rt @lizzwinstead: ben carson, may brain surgeon performed lobotomy himself. #',\n",
       " \" @nancyosborne180: last night' debate proved it! # #batsask @badassteachersa #tbats , \"]"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing symbols and words indicated in link_regex\n",
    "def strip_links(text):\n",
    "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "    links         = re.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        text = text.replace(link[0], ', ')    \n",
    "    return text\n",
    "\n",
    "#Removing punctuation\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@','#']\n",
    "    for separator in  string.punctuation:\n",
    "        if separator not in entity_prefixes :\n",
    "            text = text.replace(separator,' ')\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                words.append(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "corpus = [strip_links(x) for x in corpus]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everyone feel climate change question last night exactly',\n",
       " 'catch full last night scott best lines 90 seconds',\n",
       " 'mention tamir rice held cleveland wow',\n",
       " 'carly fiorina trending hours debate men just completed says',\n",
       " 'delivered highest ratings history presidential debates',\n",
       " 'tx on first day rescind every illegal executive action taken barack obama',\n",
       " 'liked happy heard going moderator anymore',\n",
       " 'going live around pm et',\n",
       " 'deer headlights rt ben carson may brain surgeon performed lobotomy himself',\n",
       " 'last night debate proved it']"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [strip_all_entities(x) for x in corpus]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Link, User and Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everyone feel climate change question last night exactly',\n",
       " 'catch full last night scott best lines 90 seconds',\n",
       " 'mention tamir rice held cleveland wow',\n",
       " 'carly fiorina trending hours debate men completed says',\n",
       " 'delivered highest ratings history presidential debates',\n",
       " 'tx first day rescind every illegal executive action taken barack obama',\n",
       " 'liked happy heard going moderator anymore',\n",
       " 'going live around pm et',\n",
       " 'deer headlights rt ben carson may brain surgeon performed lobotomy',\n",
       " 'last night debate proved']"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [preprocess(x) for x in corpus]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout these steps, we have been removing a lot of values. This may have created additional (unnecessary) whitespace. Just to make a proper data cleaning without creating additional noise, we will eliminate all whitespace between values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['everyone feel climate change question last night exactly',\n",
       " 'catch full last night scott best lines 90 seconds',\n",
       " 'mention tamir rice held cleveland wow',\n",
       " 'carly fiorina trending hours debate men completed says',\n",
       " 'delivered highest ratings history presidential debates',\n",
       " 'tx first day rescind every illegal executive action taken barack obama',\n",
       " 'liked happy heard going moderator anymore',\n",
       " 'going live around pm et',\n",
       " 'deer headlights rt ben carson may brain surgeon performed lobotomy',\n",
       " 'last night debate proved']"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_whitespace(in_str):\n",
    "    return in_str.strip()\n",
    "\n",
    "corpus = [remove_whitespace(x) for x in corpus]\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our new dataset cleaning process, we will apply the same process to generate our language model. As mentioned before, we think that further and more systematic data cleaning was needed to correct the problem seen in generating tweets (repetitive words). In the case where we still have this issue, we will then rework the dataset preparation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Sequence of N-Grams Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[173, 294],\n",
       " [173, 294, 274],\n",
       " [173, 294, 274, 223],\n",
       " [173, 294, 274, 223, 30],\n",
       " [173, 294, 274, 223, 30, 7],\n",
       " [173, 294, 274, 223, 30, 7, 6],\n",
       " [173, 294, 274, 223, 30, 7, 6, 770],\n",
       " [1530, 417],\n",
       " [1530, 417, 7],\n",
       " [1530, 417, 7, 6]]"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the Sequences and obtain Variables : Predictors and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTMs for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 20, 10)            106640    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10664)             1077064   \n",
      "=================================================================\n",
      "Total params: 1,228,104\n",
      "Trainable params: 1,228,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like previously, we will train our model on 1 epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "88930/88930 [==============================] - 201s 2ms/step - loss: 7.4281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a7b39f710>"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like previously, we will train our model on 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88930/88930 [==============================] - 146s 2ms/step - loss: 7.3913\n",
      "Epoch 2/100\n",
      "88930/88930 [==============================] - 145s 2ms/step - loss: 6.5038\n",
      "Epoch 3/100\n",
      "88930/88930 [==============================] - 142s 2ms/step - loss: 6.0939\n",
      "Epoch 4/100\n",
      "88930/88930 [==============================] - 140s 2ms/step - loss: 5.7728\n",
      "Epoch 5/100\n",
      "88930/88930 [==============================] - 127s 1ms/step - loss: 5.4878\n",
      "Epoch 6/100\n",
      "88930/88930 [==============================] - 133s 1ms/step - loss: 5.2319\n",
      "Epoch 7/100\n",
      "88930/88930 [==============================] - 134s 2ms/step - loss: 4.9958\n",
      "Epoch 8/100\n",
      "88930/88930 [==============================] - 138s 2ms/step - loss: 4.7738\n",
      "Epoch 9/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 4.2154\n",
      "Epoch 12/100\n",
      "88930/88930 [==============================] - 127s 1ms/step - loss: 4.0575\n",
      "Epoch 13/100\n",
      "88930/88930 [==============================] - 132s 1ms/step - loss: 3.9207\n",
      "Epoch 14/100\n",
      "88930/88930 [==============================] - 148s 2ms/step - loss: 3.7788\n",
      "Epoch 15/100\n",
      "88930/88930 [==============================] - 147s 2ms/step - loss: 3.6622\n",
      "Epoch 16/100\n",
      "88930/88930 [==============================] - 139s 2ms/step - loss: 3.5483\n",
      "Epoch 17/100\n",
      "88930/88930 [==============================] - 141s 2ms/step - loss: 3.4449\n",
      "Epoch 18/100\n",
      "88930/88930 [==============================] - 139s 2ms/step - loss: 3.3479\n",
      "Epoch 19/100\n",
      "88930/88930 [==============================] - 137s 2ms/step - loss: 3.2626\n",
      "Epoch 20/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 3.1828\n",
      "Epoch 21/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 3.1047\n",
      "Epoch 22/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 3.0374\n",
      "Epoch 23/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 2.9699\n",
      "Epoch 24/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.9038\n",
      "Epoch 25/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 2.8534\n",
      "Epoch 26/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 2.7984\n",
      "Epoch 27/100\n",
      "88930/88930 [==============================] - 128s 1ms/step - loss: 2.7490\n",
      "Epoch 28/100\n",
      "88930/88930 [==============================] - 145s 2ms/step - loss: 2.7006\n",
      "Epoch 29/100\n",
      "88930/88930 [==============================] - 181s 2ms/step - loss: 2.6594\n",
      "Epoch 30/100\n",
      "88930/88930 [==============================] - 177s 2ms/step - loss: 2.6224\n",
      "Epoch 31/100\n",
      "88930/88930 [==============================] - 177s 2ms/step - loss: 2.5807\n",
      "Epoch 32/100\n",
      "88930/88930 [==============================] - 175s 2ms/step - loss: 2.5464\n",
      "Epoch 33/100\n",
      "88930/88930 [==============================] - 158s 2ms/step - loss: 2.5103\n",
      "Epoch 34/100\n",
      "88930/88930 [==============================] - 138s 2ms/step - loss: 2.4774\n",
      "Epoch 35/100\n",
      "88930/88930 [==============================] - 139s 2ms/step - loss: 2.4446\n",
      "Epoch 36/100\n",
      "88930/88930 [==============================] - 137s 2ms/step - loss: 2.4202\n",
      "Epoch 37/100\n",
      "88930/88930 [==============================] - 134s 2ms/step - loss: 2.3899\n",
      "Epoch 38/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.3683\n",
      "Epoch 39/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 2.3419\n",
      "Epoch 40/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 2.3195\n",
      "Epoch 41/100\n",
      "88930/88930 [==============================] - 136s 2ms/step - loss: 2.2991\n",
      "Epoch 42/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 2.2758\n",
      "Epoch 43/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.2577\n",
      "Epoch 44/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.2379\n",
      "Epoch 45/100\n",
      "88930/88930 [==============================] - 129s 1ms/step - loss: 2.2174\n",
      "Epoch 46/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 2.2024\n",
      "Epoch 47/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 2.1799\n",
      "Epoch 48/100\n",
      "88930/88930 [==============================] - 138s 2ms/step - loss: 2.1690\n",
      "Epoch 49/100\n",
      "88930/88930 [==============================] - 127s 1ms/step - loss: 2.1525\n",
      "Epoch 50/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 2.1383\n",
      "Epoch 51/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 2.1235\n",
      "Epoch 52/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 2.1138\n",
      "Epoch 53/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 2.0995\n",
      "Epoch 54/100\n",
      "88930/88930 [==============================] - 127s 1ms/step - loss: 2.0854\n",
      "Epoch 55/100\n",
      "88930/88930 [==============================] - 127s 1ms/step - loss: 2.0668\n",
      "Epoch 56/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 2.0599\n",
      "Epoch 57/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.0482\n",
      "Epoch 58/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 2.0421\n",
      "Epoch 59/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 2.0245\n",
      "Epoch 60/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.0154 1s -  - ETA:\n",
      "Epoch 61/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 2.0063\n",
      "Epoch 62/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.9958\n",
      "Epoch 63/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.9835\n",
      "Epoch 64/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.9765\n",
      "Epoch 65/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.9745\n",
      "Epoch 66/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.9640\n",
      "Epoch 67/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 1.9561\n",
      "Epoch 68/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.9468\n",
      "Epoch 69/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.9400\n",
      "Epoch 70/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.9356\n",
      "Epoch 71/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.9315\n",
      "Epoch 72/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.9206\n",
      "Epoch 73/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.9183\n",
      "Epoch 74/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.9094\n",
      "Epoch 75/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.9015\n",
      "Epoch 76/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8924\n",
      "Epoch 77/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8948\n",
      "Epoch 78/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8828\n",
      "Epoch 79/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8760\n",
      "Epoch 80/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8707\n",
      "Epoch 81/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 1.8597\n",
      "Epoch 82/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8564\n",
      "Epoch 83/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8555\n",
      "Epoch 84/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8479 0s - loss\n",
      "Epoch 85/100\n",
      "88930/88930 [==============================] - 126s 1ms/step - loss: 1.8428\n",
      "Epoch 86/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8406\n",
      "Epoch 87/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8329\n",
      "Epoch 88/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8261\n",
      "Epoch 89/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8209\n",
      "Epoch 90/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 1.8132\n",
      "Epoch 91/100\n",
      "88930/88930 [==============================] - 124s 1ms/step - loss: 1.8226\n",
      "Epoch 92/100\n",
      "88930/88930 [==============================] - 123s 1ms/step - loss: 1.8156\n",
      "Epoch 93/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.8069\n",
      "Epoch 94/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.8078\n",
      "Epoch 95/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.7990\n",
      "Epoch 96/100\n",
      "88930/88930 [==============================] - 122s 1ms/step - loss: 1.7980\n",
      "Epoch 97/100\n",
      "88930/88930 [==============================] - 125s 1ms/step - loss: 1.8041\n",
      "Epoch 98/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.7974\n",
      "Epoch 99/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.7830\n",
      "Epoch 100/100\n",
      "88930/88930 [==============================] - 121s 1ms/step - loss: 1.7819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a6f49b0b8>"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when generating text based on the word trump (like in part 1), we actually generate a Tweet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this dataset is based on the 2016 GOP Debate, we have an idea of the outcome. Surprisingly, the generated Tweets represent well the output of this debate. We have tried some other words to verify the validity/proximity of the Tweets in comparison with the real life outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Like Last Night Obviously Night\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Trump\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results from 100 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump Said Megyn Ask Nine Candidates\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"Trump\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Debate Action Fox News Fed\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"debate\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Change Made 90 Issue Next\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"climate\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Always Tell Truth Said Would\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"president\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion For Tweet Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, from our two previous analysis, when generating a deep learning language model, it is necessary to make the correct data cleaning steps. We have seen that taking a systematic process and understanding the data issues at hand, help a lot with understanding the required steps, and hence, generating a proper model. Text Generation for Tweets is more difficult as most Tweets are much shorter than texts in general. Thus, it is a lot harder to generate information that is relevant and diverse. \n",
    "\n",
    "In addition, because we don't have that much data, running 1 epoch or 100 epoch both give good outputs. \n",
    "\n",
    "The results could be improved further with the following points:\n",
    "- Adding more data\n",
    "- Fine Tuning the network architecture\n",
    "- Fine Tuning the network parameters\n",
    "\n",
    "However, there are some limitations to deep learning when generating language models. The most important one, in this exercise, is that running a deep learning model is computationally expensive versus the standard NLP approach. Thus, it takes a very long time to train model. To run additional data, we would need more powerful tools. \n",
    "\n",
    "In the next notebook (3_Text_Generation), we will see a different approach to Text Generation for Tweets. Because RNNs are expensive to run, we want to compare this analysis with a more basic approach and see the differences or similarities in output. We will also apply and analyze the standard approach to longer speeches (instead of Tweets). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sourced Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
